{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hebruwu/NLP_2023/blob/main/Part2_SI/Semeval2020_11_SI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SemEval - 2020 Task 11 (Task SI)\n",
        "##Author: Ritvik Prabhu"
      ],
      "metadata": {
        "id": "ctJaezofPMrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below install the necessary libraries for the training of the model"
      ],
      "metadata": {
        "id": "bu5QPoIfIIcc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tky7De_4bB0d"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install pytorch-crf\n",
        "# !pip install seqeval[gpu]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow the instructions here: https://propaganda.qcri.org/semeval2020-task11/index.html\n",
        "\n",
        "Download the datasets as a zip file and store it in the notebook environment"
      ],
      "metadata": {
        "id": "FGuH2fyqK1Hj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp1v8YTJ_5q7"
      },
      "outputs": [],
      "source": [
        "# !unzip datasets.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries"
      ],
      "metadata": {
        "id": "0rZPu3_2Lhx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3exv4DLwPj7-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.metrics import classification_report, f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure that the GPU is available to train on"
      ],
      "metadata": {
        "id": "p5NlkZbrLmWa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGCyvRfbM_-_",
        "outputId": "d9e84297-7039-482a-ec84-1ac83c375731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The amount of data is very dense. As a result, the preprocessing step takes a very long time. To combat this, we preprocess the data once and store the preprocessed file to run our training on. For that reason, the next few cells are commented out as they are part of the initial preprocessing step. We will include the preprocessed data in the repo to avoid any sort of delays during evaluation."
      ],
      "metadata": {
        "id": "oGLVqD5rLqQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we preprocess the data by converting them to lower case and tagging each word of each data point using the BIO tagging schema."
      ],
      "metadata": {
        "id": "uQ7PKWVLMITm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHzdCR2pPk0E"
      },
      "outputs": [],
      "source": [
        "# # Function to preprocess text\n",
        "# def preprocess_text(text):\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "#     return text\n",
        "\n",
        "\n",
        "# def load_labeled_passages(set_type):\n",
        "#     text = []\n",
        "#     bio_tagged = []\n",
        "\n",
        "#     train_files = os.listdir(f\"datasets/{set_type}-articles/\")\n",
        "#     train_file_prefixes = [x.replace(\".txt\", \"\") for x in train_files]\n",
        "#     del train_files\n",
        "\n",
        "#     articles_with_labels = []\n",
        "#     for prefix in train_file_prefixes:\n",
        "#         article_file_name = f\"datasets/{set_type}-articles/{prefix}.txt\"\n",
        "#         labels_file_name = f\"datasets/{set_type}-labels-task-si/{prefix}.task-si.labels\"\n",
        "\n",
        "#         with open(article_file_name, \"r\") as article_file:\n",
        "#             with open(labels_file_name, \"r\") as labels_file:\n",
        "#                 article = article_file.read()\n",
        "#                 words = preprocess_text(article).split()\n",
        "#                 bio_text = ['O']*len(words)\n",
        "#                 text.append(words)\n",
        "#                 article_id_added = False\n",
        "#                 for line in labels_file:\n",
        "#                     id, start, end = line.split()\n",
        "#                     start = int(start)\n",
        "#                     end = int(end)\n",
        "#                     for i, word in enumerate(words):\n",
        "#                         # Check if the start of the word is within the start and end character offsets\n",
        "#                         if start <= len(' '.join(words[:i])) < end:\n",
        "#                             bio_text[i] = 'B-PROP' if len(' '.join(words[:i])) == start else 'I-PROP'\n",
        "#                 bio_tagged.append(bio_text)\n",
        "#     return {\"text\": text,\n",
        "#             \"bio_tagged\": bio_tagged,}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now load the data and preprocess them"
      ],
      "metadata": {
        "id": "0hUNa24iMcCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcUmEX9UPs3K"
      },
      "outputs": [],
      "source": [
        "# train_data = load_labeled_passages(\"train\")\n",
        "# test_data = load_labeled_passages(\"dev\")\n",
        "# df_train = pd.DataFrame(train_data)\n",
        "# df_test = pd.DataFrame(test_data)\n",
        "# df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a bidirectional hash map to access the keys and values of each BIO tag"
      ],
      "metadata": {
        "id": "-jku1K5KMWD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsu-3MQGbNkx"
      },
      "outputs": [],
      "source": [
        "labels_to_ids = {\"B-PROP\": 1, \"I-PROP\": 2, \"O\": 0}\n",
        "ids_to_labels = {1: \"B-PROP\", 0: \"O\", 2: \"I-PROP\" }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we export the the preprocessed data into a file for later and quick use"
      ],
      "metadata": {
        "id": "aUFA-rCRMpRF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfirWwDHdtdO"
      },
      "outputs": [],
      "source": [
        "# def export_to_file(export_file_path, data):\n",
        "#     with open(export_file_path, \"w\") as f:\n",
        "#         for index, row in data.iterrows():\n",
        "#             ner_tags = row[\"bio_tagged\"]\n",
        "#             tokens = row[\"text\"]\n",
        "#             ner_tags_ids = [labels_to_ids[tag] for tag in ner_tags]\n",
        "#             if len(tokens) > 0:\n",
        "#                 f.write(\n",
        "#                     str(len(tokens))\n",
        "#                     + \"\\t\"\n",
        "#                     + \"\\t\".join(tokens)\n",
        "#                     + \"\\t\"\n",
        "#                     + \"\\t\".join(map(str, ner_tags_ids))\n",
        "#                     + \"\\n\"\n",
        "#                 )\n",
        "\n",
        "\n",
        "# export_to_file(\"./data/train.txt\", df_train)\n",
        "# export_to_file(\"./data/val.txt\", df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we develop our own tokens from the given data to allow for consistent tokenization of our text. We limit the dataset to 20K unique tokens."
      ],
      "metadata": {
        "id": "U-wLeig_N2QG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMd_U2iffihY"
      },
      "outputs": [],
      "source": [
        "# all_tokens = sum(df_train[\"text\"].to_list(), [])\n",
        "# all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
        "# counter = Counter(all_tokens_array)\n",
        "\n",
        "num_tags = len(ids_to_labels)\n",
        "vocab_size = 20000\n",
        "\n",
        "# We only take (vocab_size - 2) most commons words from the training data since\n",
        "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
        "# token and another one denoting a masking token\n",
        "# vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
        "\n",
        "# import pickle\n",
        "\n",
        "# file_path = './data/vocabulary.pkl'\n",
        "\n",
        "# # Save the vocabulary list to a file using pickle\n",
        "# with open(file_path, 'wb') as file:\n",
        "#     pickle.dump(vocabulary, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the vocabulary data and preprocessed data that we just saved"
      ],
      "metadata": {
        "id": "1u9g_jeVOQBP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8TSOil_gyE_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Specify the file path where you saved the vocabulary\n",
        "file_path = 'vocabulary.pkl'\n",
        "\n",
        "# Load the vocabulary list from the file using pickle\n",
        "with open(file_path, 'rb') as file:\n",
        "    loaded_vocabulary = pickle.load(file)\n",
        "\n",
        "# Use the loaded vocabulary for StringLookup\n",
        "lookup_layer = keras.layers.StringLookup(\n",
        "    vocabulary=loaded_vocabulary\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V1GtPu3ea-8"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.TextLineDataset(\"train.txt\")\n",
        "val_data = tf.data.TextLineDataset(\"val.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example of one of the lines of the preprocessed data"
      ],
      "metadata": {
        "id": "ZTIUrfNbOcYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R7YurGKhFHM",
        "outputId": "a437d0b7-f96c-4f2a-b9cf-8fcf965be869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'287\\tnimesh\\tpatel\\tstandup\\troutine\\tcut\\tshort\\tdue\\tto\\tuncomfortable\\tjokes\\tnimesh\\tpatel\\ta\\tcomedian\\tknown\\tfor\\tbeing\\tthe\\tfirst\\tindianamerican\\twriter\\tfor\\tsaturday\\tnight\\tlive\\thad\\this\\tstandup\\troutine\\tat\\tcultureshock\\tcut\\tshort\\tearlier\\ttonight\\tdue\\tto\\tuncomfortable\\tjokes\\tcultureshock\\tan\\tevent\\thosted\\tby\\tthe\\tasian\\tamerican\\talliance\\tis\\ta\\tcharity\\tperformance\\tshowcase\\tthat\\taims\\tto\\tprovide\\ta\\tspace\\tto\\tcelebrate\\tasian\\tamerican\\texpression\\tpatel\\twas\\tone\\tof\\tthe\\tmain\\tevents\\tpromoted\\tbeforehand\\thowever\\this\\tjokes\\tquickly\\tprogressed\\tto\\tuncomfortable\\tterritory\\tincluding\\tone\\tabout\\ta\\tgay\\tblack\\tman\\twho\\tlives\\tin\\this\\tneighborhood\\tand\\thow\\tit\\tmade\\tme\\trealize\\tthat\\tbeing\\tgay\\tis\\tdefinitely\\tnot\\ta\\tchoice\\tbecause\\tno\\tone\\twants\\tto\\tbe\\tgay\\tand\\tblack\\tthe\\ttension\\tin\\tthe\\troom\\tincreased\\tas\\tpatel\\ttold\\tmore\\tjokes\\tin\\tthis\\tvein\\tuntil\\torganizers\\tof\\tthe\\tevent\\twent\\tup\\ton\\tstage\\tto\\tstop\\thim\\tciting\\ta\\tchange\\tin\\tprogram\\tplans\\tpatel\\tquestioned\\twhy\\tthis\\twas\\thappening\\tthe\\torganizers\\treplied\\tthat\\tthe\\tperson\\tin\\tcharge\\tof\\ttech\\thad\\tto\\tleave\\tearly\\tbut\\tpatel\\tcontinued\\tto\\tclaim\\tthat\\the\\twas\\tbeing\\tcut\\toff\\tbecause\\tthe\\taudience\\tdidnt\\tlike\\this\\tjokes\\tat\\tone\\tpoint\\tone\\torganizer\\ttold\\tpatel\\the\\twas\\tbeing\\tdisrespectful\\twhen\\tasked\\tfor\\tclosing\\tremarks\\tpatel\\tresponded\\tim\\ta\\tgeneration\\tolder\\tthan\\tall\\tof\\tyou\\ti\\tknow\\tcomedy\\tand\\tcalled\\tthe\\torganizers\\tincorrect\\tin\\tending\\this\\tset\\the\\ttried\\tto\\tcontinue\\tspeaking\\tuntil\\this\\tmic\\twas\\tcut\\twe\\thave\\treached\\tout\\tto\\tthe\\tasian\\tamerican\\talliance\\tfor\\tcomment\\tand\\twere\\ttold\\tthat\\ttheir\\tboard\\tis\\tnot\\tyet\\tprepared\\tto\\trelease\\ta\\tstatement\\twe\\twill\\tupdate\\tthis\\tpost\\tif\\tsuch\\ta\\tstatement\\tis\\tforthcoming\\timage\\tvia\\tcolumbia\\tuniversity\\tasian\\tamerican\\talliance\\ttags\\tbreaking\\tcolumbia\\tuniversity\\tasian\\tamerican\\talliance\\tcultureshock\\tnimesh\\tpatel\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t2\\t2\\t2\\t2\\t2\\t2\\t2\\t2\\t2\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t2\\t2\\t2\\t2\\t2\\t2\\t2\\t2\\t2\\t2\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0']\n"
          ]
        }
      ],
      "source": [
        "print(list(train_data.take(1).as_numpy_iterator()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the Transformer layer, the Tokens and Position layer and the custom NER model below"
      ],
      "metadata": {
        "id": "y-rA7KcFOgzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu6XeHxAfHZ7"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                keras.layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvdyEJPBfKKP"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        maxlen = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        token_embeddings = self.token_emb(inputs)\n",
        "        return token_embeddings + position_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky04yJtSfMM8"
      },
      "outputs": [],
      "source": [
        "class NERModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, num_tags, vocab_size, maxlen=len(loaded_vocabulary), embed_dim=32, num_heads=2, ff_dim=32\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.ff_final(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We convert the text to a format suitable to pass to the model"
      ],
      "metadata": {
        "id": "n0DrinnZO12h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQZcuzZ5hNdT"
      },
      "outputs": [],
      "source": [
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
        "    tokens = record[1 : length + 1]\n",
        "    tags = record[length + 1 :]\n",
        "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
        "    return tokens, tags\n",
        "\n",
        "\n",
        "def lowercase_and_convert_to_ids(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "\n",
        "# We use `padded_batch` here because each record in the dataset has a\n",
        "# different length.\n",
        "batch_size = 1\n",
        "train_dataset = (\n",
        "    train_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "val_dataset = (\n",
        "    val_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "\n",
        "ner_model = NERModel(num_tags, vocab_size + 1, embed_dim=32, num_heads=4, ff_dim=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compile and train the model below"
      ],
      "metadata": {
        "id": "dqUTuzTwO6qP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx1Z0S6thTVA",
        "outputId": "5f87ea3c-50c1-42ea-debd-d575e1f2f32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "371/371 [==============================] - 151s 357ms/step - loss: 1.2667\n",
            "Epoch 2/20\n",
            "371/371 [==============================] - 6s 15ms/step - loss: 0.6088\n",
            "Epoch 3/20\n",
            "371/371 [==============================] - 4s 12ms/step - loss: 0.4736\n",
            "Epoch 4/20\n",
            "371/371 [==============================] - 5s 14ms/step - loss: 0.4425\n",
            "Epoch 5/20\n",
            "371/371 [==============================] - 5s 12ms/step - loss: 0.4287\n",
            "Epoch 6/20\n",
            "371/371 [==============================] - 6s 15ms/step - loss: 0.4198\n",
            "Epoch 7/20\n",
            "371/371 [==============================] - 5s 12ms/step - loss: 0.4146\n",
            "Epoch 8/20\n",
            "371/371 [==============================] - 5s 14ms/step - loss: 0.4098\n",
            "Epoch 9/20\n",
            "371/371 [==============================] - 4s 12ms/step - loss: 0.4071\n",
            "Epoch 10/20\n",
            "371/371 [==============================] - 5s 13ms/step - loss: 0.4052\n",
            "Epoch 11/20\n",
            "371/371 [==============================] - 5s 14ms/step - loss: 0.4033\n",
            "Epoch 12/20\n",
            "371/371 [==============================] - 4s 12ms/step - loss: 0.4019\n",
            "Epoch 13/20\n",
            "371/371 [==============================] - 5s 14ms/step - loss: 0.4006\n",
            "Epoch 14/20\n",
            "371/371 [==============================] - 5s 13ms/step - loss: 0.3994\n",
            "Epoch 15/20\n",
            "371/371 [==============================] - 4s 12ms/step - loss: 0.3983\n",
            "Epoch 16/20\n",
            "371/371 [==============================] - 5s 14ms/step - loss: 0.3978\n",
            "Epoch 17/20\n",
            "371/371 [==============================] - 5s 12ms/step - loss: 0.3968\n",
            "Epoch 18/20\n",
            "371/371 [==============================] - 5s 12ms/step - loss: 0.3959\n",
            "Epoch 19/20\n",
            "371/371 [==============================] - 5s 14ms/step - loss: 0.3951\n",
            "Epoch 20/20\n",
            "371/371 [==============================] - 5s 12ms/step - loss: 0.3944\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bcfc165e830>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=1e-5, clipvalue=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "ner_model.compile(optimizer=optimizer, loss=loss)\n",
        "ner_model.fit(train_dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We test the model's performance on the validation data"
      ],
      "metadata": {
        "id": "JIswgFf7PCKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation dataset\n",
        "val_predictions = ner_model.predict(val_dataset)\n",
        "\n",
        "# Convert predictions and true labels to flat lists\n",
        "flat_val_predictions = np.concatenate([np.argmax(pred, axis=-1).flatten() for pred in val_predictions])\n",
        "flat_val_labels = np.concatenate([y.numpy().flatten() for x, y in val_dataset])\n",
        "\n",
        "# Create a classification report\n",
        "class_report = classification_report(\n",
        "    flat_val_labels, flat_val_predictions, target_names=list(ids_to_labels.values()), output_dict=True, zero_division=0\n",
        ")\n",
        "\n",
        "# Print the classification report\n",
        "for category, metrics in class_report.items():\n",
        "    if isinstance(metrics, dict) and category == 'macro avg':\n",
        "        print(f'Category: {category}')\n",
        "        print(f'Precision: {metrics[\"precision\"]:.4f}')\n",
        "        print(f'Recall: {metrics[\"recall\"]:.4f}')\n",
        "        print(f'F1 Score: {metrics[\"f1-score\"]:.4f}')\n",
        "        print(f'Support: {metrics[\"support\"]}')\n",
        "        print('-' * 40)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNkU0BvfZzCr",
        "outputId": "0180e16b-3fd1-4260-c341-523c95050089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 1s 7ms/step\n",
            "Category: macro avg\n",
            "Precision: 0.2997\n",
            "Recall: 0.3333\n",
            "F1 Score: 0.3156\n",
            "Support: 57395\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the model below and store it as a zip file for easy transportation of the model"
      ],
      "metadata": {
        "id": "J-_HWQbXPFp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'SI_task_model'\n",
        "ner_model.save(model_save_path)\n",
        "lookup_layer_save_path = 'lookup_layer'\n",
        "with open(lookup_layer_save_path, 'wb') as file:\n",
        "    pickle.dump(lookup_layer.get_vocabulary(), file)"
      ],
      "metadata": {
        "id": "iv0gYMFIrguu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "folder_to_zip = 'SI_task_model'\n",
        "zip_file_name = 'SI_task_model_files.zip'\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f'The folder \"{folder_to_zip}\" has been zipped into \"{zip_file_name}\".')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziLRiuBa5eBi",
        "outputId": "64f4d808-5eaf-4172-a673-f7f7186473e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The folder \"SI_task_model\" has been zipped into \"SI_task_model_files.zip\".\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}